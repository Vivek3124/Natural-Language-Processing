{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f4198b",
   "metadata": {},
   "source": [
    "# Importing libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f854abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "import itertools\n",
    "from heapq import nlargest\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "trdf = pd.read_csv(\"qc_train.txt\", delimiter = \":\",header=None, names=['Class', 'Question'])\n",
    "\n",
    "# Removing the fine grained categories label from Questions, and cleaning the questions\n",
    "ques = []\n",
    "for i in range(len(trdf)):\n",
    "    temp = trdf['Question'][i]\n",
    "    temp = temp.split(\" \",1)[1]\n",
    "    temp = re.sub(r\"[^a-zA-Z0-9.'\\s]\", ' ',temp)\n",
    "    ques.append(temp)\n",
    "\n",
    "trdf['Question'] = ques\n",
    "\n",
    "tsdf = pd.read_csv(\"qc_test.txt\", delimiter = \":\",header=None, names=['Class', 'Question'])\n",
    "\n",
    "# Removing the fine grained categories label from Questions, and cleaning the questions\n",
    "ques = []\n",
    "for i in range(len(tsdf)):\n",
    "    temp = tsdf['Question'][i]\n",
    "    temp = temp.split(\" \",1)[1]\n",
    "    temp = re.sub(r\"[^a-zA-Z0-9.'\\s]\", ' ',temp)\n",
    "    ques.append(temp)\n",
    "\n",
    "tsdf['Question'] = ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4420b5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DESC</td>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTY</td>\n",
       "      <td>What films featured the character Popeye Doyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DESC</td>\n",
       "      <td>How can I find a list of celebrities real names</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTY</td>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBR</td>\n",
       "      <td>What is the full form of .com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class                                           Question\n",
       "0  DESC  How did serfdom develop in and then leave Russ...\n",
       "1  ENTY   What films featured the character Popeye Doyle  \n",
       "2  DESC  How can I find a list of celebrities real names  \n",
       "3  ENTY  What fowl grabs the spotlight after the Chines...\n",
       "4  ABBR                    What is the full form of .com  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "841eb8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NUM</td>\n",
       "      <td>How far is it from Denver to Aspen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOC</td>\n",
       "      <td>What county is Modesto   California in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HUM</td>\n",
       "      <td>Who was Galileo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DESC</td>\n",
       "      <td>What is an atom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NUM</td>\n",
       "      <td>When did Hawaii become a state</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class                                  Question\n",
       "0   NUM      How far is it from Denver to Aspen  \n",
       "1   LOC  What county is Modesto   California in  \n",
       "2   HUM                         Who was Galileo  \n",
       "3  DESC                         What is an atom  \n",
       "4   NUM          When did Hawaii become a state  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3062ba8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5952"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting total dataset\n",
    "new_index = [i for i in range(5452,5952)]\n",
    "ts = tsdf.copy()\n",
    "ts.index = new_index\n",
    "frames = [trdf, ts]\n",
    "total_df = pd.concat(frames)\n",
    "len(total_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982ac077",
   "metadata": {},
   "source": [
    "## Get class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd73ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data class distribution [('ABBR', 0.02), ('DESC', 0.21), ('ENTY', 0.23), ('HUM', 0.22), ('LOC', 0.15), ('NUM', 0.16)]\n",
      "Testing Data class distribution [('ABBR', 0.02), ('DESC', 0.28), ('ENTY', 0.19), ('HUM', 0.13), ('LOC', 0.16), ('NUM', 0.23)]\n"
     ]
    }
   ],
   "source": [
    "def get_dist_cls(df):\n",
    "    my_list = list(df['Class'])\n",
    "    freq_class = {}\n",
    "    for item in my_list:\n",
    "        if (item in freq_class):\n",
    "            freq_class[item] += 1\n",
    "        else:\n",
    "            freq_class[item] = 1\n",
    "\n",
    "    sm = sum(freq_class.values())\n",
    "    for key in freq_class:\n",
    "        freq_class[key] = round(freq_class[key]/sm,2)\n",
    "    freq_class = sorted(freq_class.items(), key=lambda item: item[0])\n",
    "    return (freq_class)\n",
    "\n",
    "print(\"Training Data class distribution\",get_dist_cls(trdf))\n",
    "print(\"Testing Data class distribution\",get_dist_cls(tsdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a29a5c",
   "metadata": {},
   "source": [
    "# Function to get features and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e8fde89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(trdf):\n",
    "    # Creating lexical features i.e. ngrams for each question\n",
    "    lex_f_1gram = []\n",
    "    lex_f_2gram = []\n",
    "    lex_f_3gram = []\n",
    "\n",
    "    # Creating a count hashmap/dict. for all ngrams\n",
    "    lex_f1_dic = {}\n",
    "    lex_f2_dic = {}\n",
    "    lex_f3_dic = {}\n",
    "\n",
    "    # length feature\n",
    "    length_q = []\n",
    "\n",
    "    # syntactical features i.e part of speech for each question\n",
    "    pos_lol = []\n",
    "\n",
    "    # Creating a count hashmap/dict. for all pos\n",
    "    pos_dic = {}\n",
    "\n",
    "\n",
    "    # Iterating for all training questions\n",
    "\n",
    "    for i in range(len(trdf)):\n",
    "        \n",
    "        tokens = nltk.word_tokenize(trdf.loc[i, \"Question\"])  # generating tokens from question using the word_tokenize() in nltk\n",
    "        \n",
    "        # creating list of ngrams respectively\n",
    "        \n",
    "        sequences_1 = [tokens[i:] for i in range(1)]\n",
    "        grams_1 = zip(*sequences_1)\n",
    "        \n",
    "        sequences_2 = [tokens[i:] for i in range(2)]\n",
    "        grams_2 = zip(*sequences_2)\n",
    "        \n",
    "        sequences_3 = [tokens[i:] for i in range(3)]\n",
    "        grams_3 = zip(*sequences_3)\n",
    "        \n",
    "        # appending the list of ngrams in respective list of lists\n",
    "        lex_f_1gram.append(list(grams_1))\n",
    "        lex_f_2gram.append(list(grams_2)) \n",
    "        lex_f_3gram.append(list(grams_3))\n",
    "        \n",
    "        pos_list = nltk.pos_tag(tokens)      # getting the tag for each token using pos_tag() function in nltk library\n",
    "        pos_lol.append(pos_list)             # appending the list of tags in pos list of lists\n",
    "        \n",
    "        length_q.append(len(lex_f_1gram[i])) # appending the length of a each question\n",
    "        \n",
    "        \n",
    "        # Making dictionary of lexical ngrams and syntactic part of speech\n",
    "        for pos in pos_list:\n",
    "            if pos[1] in pos_dic: \n",
    "                pos_dic[pos[1]] += 1\n",
    "            else:\n",
    "                pos_dic[pos[1]] = 1\n",
    "        \n",
    "        for word in lex_f_1gram[i]:\n",
    "            if word in lex_f1_dic.keys():\n",
    "                lex_f1_dic[word] += 1\n",
    "            else:\n",
    "                lex_f1_dic[word] = 1\n",
    "                \n",
    "        for word in lex_f_2gram[i]:\n",
    "            if word in lex_f2_dic.keys():\n",
    "                lex_f2_dic[word] += 1\n",
    "            else:\n",
    "                lex_f2_dic[word] = 1\n",
    "            \n",
    "        for word in lex_f_3gram[i]:\n",
    "            if word in lex_f3_dic.keys():\n",
    "                lex_f3_dic[word] += 1\n",
    "            else:\n",
    "                lex_f3_dic[word] = 1\n",
    "\n",
    "    # Getting the total count of items in lexical features for each ngram and syntactic features\n",
    "    values = pos_dic.values() \n",
    "    len_synt=sum(values)\n",
    "\n",
    "    values = lex_f1_dic.values() \n",
    "    len_1gram=sum(values)\n",
    "\n",
    "    values = lex_f2_dic.values() \n",
    "    len_2gram=sum(values)\n",
    "\n",
    "    values = lex_f3_dic.values() \n",
    "    len_3gram=sum(values)\n",
    "\n",
    "    # getting most frequent ngrams (500,300,200) for 1gram, 2gram, 3gram respectively\n",
    "    lex_f1_500 = nlargest(500, lex_f1_dic, key = lex_f1_dic.get)\n",
    "    lex_f2_300 = nlargest(300, lex_f2_dic, key = lex_f2_dic.get)\n",
    "    lex_f3_200 = nlargest(200, lex_f3_dic, key = lex_f3_dic.get)\n",
    "\n",
    "    # Marking the presence or absence of ngrams in dictionary to make furthur calculations fast\n",
    "    gram1 = {}\n",
    "    gram2 = {}\n",
    "    gram3 = {}\n",
    "\n",
    "    for key in lex_f1_dic:\n",
    "        if key in lex_f1_500:\n",
    "            gram1[key] = 1\n",
    "        else:\n",
    "            gram1[key] = 0\n",
    "\n",
    "    for key in lex_f2_dic:\n",
    "        if key in lex_f2_300:\n",
    "            gram2[key] = 1\n",
    "        else:\n",
    "            gram2[key] = 0\n",
    "\n",
    "    for key in lex_f3_dic:\n",
    "        if key in lex_f3_200:\n",
    "            gram3[key] = 1\n",
    "        else:\n",
    "            gram3[key] = 0\n",
    "\n",
    "    # list of syntactic tags for 1gram present in most frequent 500 1grams and list of their corresponding scores\n",
    "    Syntactic = []\n",
    "    synt_scores = []\n",
    "\n",
    "    # lists of lexical ngrams which are present in most frequent 500,300,200 ngrams respectively and lists of their scores\n",
    "    Lexical_1gram = []\n",
    "    Lexical_2gram = []\n",
    "    Lexical_3gram = []\n",
    "\n",
    "    lex_scores_1gram = []\n",
    "    lex_scores_2gram = []\n",
    "    lex_scores_3gram = []\n",
    "\n",
    "\n",
    "\n",
    "    # Iterating for all training questions to compute above stated lists\n",
    "    for i in range(len(trdf)):\n",
    "        \n",
    "        score_si = 0     # var to store syntactic prob score\n",
    "        score_li = 0     # var to store lexical prob score\n",
    "        \n",
    "        # initialising empty lists to store 1gram and pos tags\n",
    "        temp1 = []\n",
    "        temp_S =[]\n",
    "        \n",
    "        for key in lex_f_1gram[i]:\n",
    "            \n",
    "            if gram1[key] != 0:      # checking weather given key(1gram) is present in most frequent 500 or not\n",
    "                temp1.append(key[0])\n",
    "                \n",
    "                # computing probability score for 1gram\n",
    "                score_li += math.log(lex_f1_dic[key]/len_1gram)\n",
    "                \n",
    "                # iterating over each questions part of speech list\n",
    "                for tup in pos_lol[i]:\n",
    "                    \n",
    "                    if tup[0] == key[0]:      # checking wheather pos instance is same as 1gram in consideration\n",
    "                        temp_S.append(tup[1]) # tup[1] is tag assigned corresponding to 1gram\n",
    "                        \n",
    "                        # computing probability score for part of speech tags\n",
    "                        score_si += math.log(pos_dic[tup[1]]/len_synt)\n",
    "        \n",
    "        # appending scores, 1grams and pos tags\n",
    "        synt_scores.append(abs(score_si))\n",
    "        lex_scores_1gram.append(abs(score_li))\n",
    "        \n",
    "        Lexical_1gram.append(temp1)\n",
    "        Syntactic.append(temp_S)\n",
    "        \n",
    "        \n",
    "        # initialising empty list to store 2gram\n",
    "        temp2 = []\n",
    "        \n",
    "        score_li = 0\n",
    "        for key in lex_f_2gram[i]:\n",
    "            \n",
    "            if gram2[key] != 0:     # checking weather given key(2gram) is present in most frequent 300 or not\n",
    "                temp2.append(key)\n",
    "                \n",
    "                # computing conditional probability score for 2gram\n",
    "                p1 = lex_f2_dic[key]/len_2gram\n",
    "                p2 = lex_f1_dic[(key[0],)]/len_1gram         # (What,is) -> key, P(is|what) = P(What,is)/P(what)\n",
    "                score_li += math.log(p1/p2)\n",
    "        \n",
    "        # appending scores and 2grams\n",
    "        lex_scores_2gram.append(abs(score_li))\n",
    "        Lexical_2gram.append(temp2)\n",
    "        \n",
    "        \n",
    "        # initialising empty list to store 3gram\n",
    "        temp3 = []\n",
    "        \n",
    "        score_li = 0\n",
    "        for key in lex_f_3gram[i]:\n",
    "            if gram3[key] != 0:     # checking weather given key(3gram) is present in most frequent 200 or not\n",
    "                temp3.append(key)\n",
    "                \n",
    "                # computing conditional probability score for 3gram\n",
    "                # (what, is, the) -> key ,\n",
    "                \n",
    "                p1 = lex_f3_dic[key]/len_3gram              #  p1 = P(what, is, the)\n",
    "                p2 = lex_f2_dic[(key[0],key[1])]/len_2gram  #  p2 = P(What,is)\n",
    "                p3 = lex_f1_dic[(key[0],)]/len_1gram        #  p1 = P(what)\n",
    "              \n",
    "                # P(is|what) = p2/p1\n",
    "                \n",
    "                # final =  P(the|what,is) = P(what, is, the)/P(is|what)\n",
    "                \n",
    "                score_li += math.log(p1/(p2/p3))\n",
    "        \n",
    "        # appending scores and 3grams\n",
    "        lex_scores_3gram.append(abs(score_li))\n",
    "        Lexical_3gram.append(temp3)\n",
    "    # Creating new columns of features\n",
    "    trdf['Length'] = length_q\n",
    "    trdf['Lexical_1gram'] = Lexical_1gram\n",
    "    trdf['Lex_Prob_Scores_1gram'] = lex_scores_1gram\n",
    "    trdf['Lexical_2gram'] = Lexical_2gram\n",
    "    trdf['Lex_Prob_Scores_2gram'] = lex_scores_2gram\n",
    "    trdf['Lexical_3gram'] = Lexical_3gram\n",
    "    trdf['Lex_Prob_Scores_3gram'] = lex_scores_3gram\n",
    "    trdf['Syntactic'] = Syntactic\n",
    "    trdf['Syntactic_Prob_Scores'] = synt_scores\n",
    "    return trdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072370cc",
   "metadata": {},
   "source": [
    "## defining values for features using create_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf3f1782",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Question</th>\n",
       "      <th>Length</th>\n",
       "      <th>Lexical_1gram</th>\n",
       "      <th>Lex_Prob_Scores_1gram</th>\n",
       "      <th>Lexical_2gram</th>\n",
       "      <th>Lex_Prob_Scores_2gram</th>\n",
       "      <th>Lexical_3gram</th>\n",
       "      <th>Lex_Prob_Scores_3gram</th>\n",
       "      <th>Syntactic</th>\n",
       "      <th>Syntactic_Prob_Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DESC</td>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[How, did, in, and, then]</td>\n",
       "      <td>26.007478</td>\n",
       "      <td>[(How, did)]</td>\n",
       "      <td>3.226824</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[WRB, VBD, IN, CC, RB]</td>\n",
       "      <td>18.242043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTY</td>\n",
       "      <td>What films featured the character Popeye Doyle</td>\n",
       "      <td>7</td>\n",
       "      <td>[What, the, character]</td>\n",
       "      <td>12.625627</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[WP, DT, NN]</td>\n",
       "      <td>6.475343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DESC</td>\n",
       "      <td>How can I find a list of celebrities real names</td>\n",
       "      <td>10</td>\n",
       "      <td>[How, can, I, find, a, list, of, real, names]</td>\n",
       "      <td>53.465525</td>\n",
       "      <td>[(How, can), (can, I), (I, find), (find, a)]</td>\n",
       "      <td>5.339126</td>\n",
       "      <td>[(How, can, I), (can, I, find), (I, find, a)]</td>\n",
       "      <td>17.262693</td>\n",
       "      <td>[WRB, MD, PRP, VB, DT, NN, IN, JJ, NNS]</td>\n",
       "      <td>29.197039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTY</td>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "      <td>12</td>\n",
       "      <td>[What, the, after, the, Chinese, of, the]</td>\n",
       "      <td>29.855507</td>\n",
       "      <td>[(of, the)]</td>\n",
       "      <td>1.126969</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[WP, DT, DT, DT, IN, DT, DT, DT, JJ, IN, DT, D...</td>\n",
       "      <td>30.417258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBR</td>\n",
       "      <td>What is the full form of .com</td>\n",
       "      <td>7</td>\n",
       "      <td>[What, is, the, full, form, of]</td>\n",
       "      <td>28.578077</td>\n",
       "      <td>[(What, is), (is, the)]</td>\n",
       "      <td>1.670732</td>\n",
       "      <td>[(What, is, the)]</td>\n",
       "      <td>3.132457</td>\n",
       "      <td>[WP, VBZ, DT, JJ, NN, IN]</td>\n",
       "      <td>14.477990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class                                           Question  Length  \\\n",
       "0  DESC  How did serfdom develop in and then leave Russ...       9   \n",
       "1  ENTY   What films featured the character Popeye Doyle         7   \n",
       "2  DESC  How can I find a list of celebrities real names        10   \n",
       "3  ENTY  What fowl grabs the spotlight after the Chines...      12   \n",
       "4  ABBR                    What is the full form of .com         7   \n",
       "\n",
       "                                   Lexical_1gram  Lex_Prob_Scores_1gram  \\\n",
       "0                      [How, did, in, and, then]              26.007478   \n",
       "1                         [What, the, character]              12.625627   \n",
       "2  [How, can, I, find, a, list, of, real, names]              53.465525   \n",
       "3      [What, the, after, the, Chinese, of, the]              29.855507   \n",
       "4                [What, is, the, full, form, of]              28.578077   \n",
       "\n",
       "                                  Lexical_2gram  Lex_Prob_Scores_2gram  \\\n",
       "0                                  [(How, did)]               3.226824   \n",
       "1                                            []               0.000000   \n",
       "2  [(How, can), (can, I), (I, find), (find, a)]               5.339126   \n",
       "3                                   [(of, the)]               1.126969   \n",
       "4                       [(What, is), (is, the)]               1.670732   \n",
       "\n",
       "                                   Lexical_3gram  Lex_Prob_Scores_3gram  \\\n",
       "0                                             []               0.000000   \n",
       "1                                             []               0.000000   \n",
       "2  [(How, can, I), (can, I, find), (I, find, a)]              17.262693   \n",
       "3                                             []               0.000000   \n",
       "4                              [(What, is, the)]               3.132457   \n",
       "\n",
       "                                           Syntactic  Syntactic_Prob_Scores  \n",
       "0                             [WRB, VBD, IN, CC, RB]              18.242043  \n",
       "1                                       [WP, DT, NN]               6.475343  \n",
       "2            [WRB, MD, PRP, VB, DT, NN, IN, JJ, NNS]              29.197039  \n",
       "3  [WP, DT, DT, DT, IN, DT, DT, DT, JJ, IN, DT, D...              30.417258  \n",
       "4                          [WP, VBZ, DT, JJ, NN, IN]              14.477990  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create features for total dataset\n",
    "total_df = create_features(total_df)\n",
    "total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c784f562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Question</th>\n",
       "      <th>Lexical_1gram</th>\n",
       "      <th>Lexical_2gram</th>\n",
       "      <th>Lexical_3gram</th>\n",
       "      <th>Syntactic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DESC</td>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "      <td>[How, did, in, and, then]</td>\n",
       "      <td>[(How, did)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[WRB, VBD, IN, CC, RB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTY</td>\n",
       "      <td>What films featured the character Popeye Doyle</td>\n",
       "      <td>[What, the, character]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[WP, DT, NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DESC</td>\n",
       "      <td>How can I find a list of celebrities real names</td>\n",
       "      <td>[How, can, I, find, a, list, of, real, names]</td>\n",
       "      <td>[(How, can), (can, I), (I, find), (find, a)]</td>\n",
       "      <td>[(How, can, I), (can, I, find), (I, find, a)]</td>\n",
       "      <td>[WRB, MD, PRP, VB, DT, NN, IN, JJ, NNS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTY</td>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "      <td>[What, the, after, the, Chinese, of, the]</td>\n",
       "      <td>[(of, the)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[WP, DT, DT, DT, IN, DT, DT, DT, JJ, IN, DT, D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBR</td>\n",
       "      <td>What is the full form of .com</td>\n",
       "      <td>[What, is, the, full, form, of]</td>\n",
       "      <td>[(What, is), (is, the)]</td>\n",
       "      <td>[(What, is, the)]</td>\n",
       "      <td>[WP, VBZ, DT, JJ, NN, IN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>ENTY</td>\n",
       "      <td>What's the shape of a camel's spine</td>\n",
       "      <td>[What, 's, the, of, a, 's]</td>\n",
       "      <td>[(What, 's), ('s, the), (of, a)]</td>\n",
       "      <td>[(What, 's, the)]</td>\n",
       "      <td>[WP, VBZ, POS, DT, IN, DT, VBZ, POS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>ENTY</td>\n",
       "      <td>What type of currency is used in China</td>\n",
       "      <td>[What, type, of, is, used, in, China]</td>\n",
       "      <td>[(What, type), (type, of), (used, in)]</td>\n",
       "      <td>[(What, type, of)]</td>\n",
       "      <td>[WP, NN, IN, VBZ, VBN, IN, NNP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5449</th>\n",
       "      <td>NUM</td>\n",
       "      <td>What is the temperature today</td>\n",
       "      <td>[What, is, the]</td>\n",
       "      <td>[(What, is), (is, the)]</td>\n",
       "      <td>[(What, is, the)]</td>\n",
       "      <td>[WP, VBZ, DT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>NUM</td>\n",
       "      <td>What is the temperature for cooking</td>\n",
       "      <td>[What, is, the, for]</td>\n",
       "      <td>[(What, is), (is, the)]</td>\n",
       "      <td>[(What, is, the)]</td>\n",
       "      <td>[WP, VBZ, DT, IN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5451</th>\n",
       "      <td>ENTY</td>\n",
       "      <td>What currency is used in Australia</td>\n",
       "      <td>[What, is, used, in, Australia]</td>\n",
       "      <td>[(used, in)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[WP, VBZ, VBN, IN, NNP]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5452 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Class                                           Question  \\\n",
       "0     DESC  How did serfdom develop in and then leave Russ...   \n",
       "1     ENTY   What films featured the character Popeye Doyle     \n",
       "2     DESC  How can I find a list of celebrities real names     \n",
       "3     ENTY  What fowl grabs the spotlight after the Chines...   \n",
       "4     ABBR                    What is the full form of .com     \n",
       "...    ...                                                ...   \n",
       "5447  ENTY              What's the shape of a camel's spine     \n",
       "5448  ENTY           What type of currency is used in China     \n",
       "5449   NUM                    What is the temperature today     \n",
       "5450   NUM              What is the temperature for cooking     \n",
       "5451  ENTY               What currency is used in Australia     \n",
       "\n",
       "                                      Lexical_1gram  \\\n",
       "0                         [How, did, in, and, then]   \n",
       "1                            [What, the, character]   \n",
       "2     [How, can, I, find, a, list, of, real, names]   \n",
       "3         [What, the, after, the, Chinese, of, the]   \n",
       "4                   [What, is, the, full, form, of]   \n",
       "...                                             ...   \n",
       "5447                     [What, 's, the, of, a, 's]   \n",
       "5448          [What, type, of, is, used, in, China]   \n",
       "5449                                [What, is, the]   \n",
       "5450                           [What, is, the, for]   \n",
       "5451                [What, is, used, in, Australia]   \n",
       "\n",
       "                                     Lexical_2gram  \\\n",
       "0                                     [(How, did)]   \n",
       "1                                               []   \n",
       "2     [(How, can), (can, I), (I, find), (find, a)]   \n",
       "3                                      [(of, the)]   \n",
       "4                          [(What, is), (is, the)]   \n",
       "...                                            ...   \n",
       "5447              [(What, 's), ('s, the), (of, a)]   \n",
       "5448        [(What, type), (type, of), (used, in)]   \n",
       "5449                       [(What, is), (is, the)]   \n",
       "5450                       [(What, is), (is, the)]   \n",
       "5451                                  [(used, in)]   \n",
       "\n",
       "                                      Lexical_3gram  \\\n",
       "0                                                []   \n",
       "1                                                []   \n",
       "2     [(How, can, I), (can, I, find), (I, find, a)]   \n",
       "3                                                []   \n",
       "4                                 [(What, is, the)]   \n",
       "...                                             ...   \n",
       "5447                              [(What, 's, the)]   \n",
       "5448                             [(What, type, of)]   \n",
       "5449                              [(What, is, the)]   \n",
       "5450                              [(What, is, the)]   \n",
       "5451                                             []   \n",
       "\n",
       "                                              Syntactic  \n",
       "0                                [WRB, VBD, IN, CC, RB]  \n",
       "1                                          [WP, DT, NN]  \n",
       "2               [WRB, MD, PRP, VB, DT, NN, IN, JJ, NNS]  \n",
       "3     [WP, DT, DT, DT, IN, DT, DT, DT, JJ, IN, DT, D...  \n",
       "4                             [WP, VBZ, DT, JJ, NN, IN]  \n",
       "...                                                 ...  \n",
       "5447               [WP, VBZ, POS, DT, IN, DT, VBZ, POS]  \n",
       "5448                    [WP, NN, IN, VBZ, VBN, IN, NNP]  \n",
       "5449                                      [WP, VBZ, DT]  \n",
       "5450                                  [WP, VBZ, DT, IN]  \n",
       "5451                            [WP, VBZ, VBN, IN, NNP]  \n",
       "\n",
       "[5452 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df.iloc[0:5452,[0,1,3,5,7,9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca65794b",
   "metadata": {},
   "source": [
    "# Creating Utillity functions and classes for tree nodes and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9ef982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring distribution of values in features\n",
    "def explore_features(df):\n",
    "    cols = list(df.columns)\n",
    "    cols.pop()\n",
    "    for col in cols:\n",
    "        x = df.loc[:,col]\n",
    "        print(\"For\",col)\n",
    "        print(\"\\tMin:\",np.min(x))\n",
    "        print(\"\\tMax:\",np.max(x))\n",
    "        print(\"\\tUnique:\",len(np.unique(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1a78fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get training and validation set indexes for k-fold\n",
    "def apply_k_fold(df,n):\n",
    "    # prepare cross validation\n",
    "    kfold = KFold(n)\n",
    "    train_set = []\n",
    "    validn_set = []\n",
    "\n",
    "    # enumerate splits\n",
    "    for train, validn in kfold.split(df):\n",
    "        train_set.append(train)\n",
    "        validn_set.append(validn)\n",
    "    \n",
    "    return (train_set,validn_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a29bfe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree node class\n",
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
    "        ''' constructor ''' \n",
    "        \n",
    "        # for decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        \n",
    "        # for leaf node\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a4bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier class\n",
    "class DecisionTreeClassifier():\n",
    "    def __init__(self, min_samples_split, max_depth):\n",
    "        ''' constructor '''\n",
    "        \n",
    "        # initialize the root of the tree \n",
    "        self.root = None\n",
    "        \n",
    "        # stopping conditions\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.itr = 0\n",
    "        \n",
    "    def build_tree(self, dataset, model, curr_depth):\n",
    "        ''' recursive function to build the tree ''' \n",
    "        \n",
    "        X, Y = dataset.iloc[:,:-1], dataset.iloc[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        uniq = len(np.unique(Y))\n",
    "        # split until stopping conditions are met\n",
    "        if num_samples >= self.min_samples_split and uniq > 1 and curr_depth < self.max_depth:\n",
    "            \n",
    "            # find the best split\n",
    "            print(\"Splitting for\",self.itr,\"th node\")\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features,model)\n",
    "            \n",
    "            print(\"\\n\\n\\n**************************************************\\n\\n\\n\")\n",
    "            \n",
    "            # check if information gain is positive\n",
    "            if best_split[\"info_gain\"] > 0:\n",
    "                \n",
    "                self.itr += 1\n",
    "                # recur left\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], model, curr_depth+1)\n",
    "                \n",
    "                self.itr += 1\n",
    "                # recur right\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], model, curr_depth+1)\n",
    "                \n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "        \n",
    "        # compute leaf node\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        # return leaf node\n",
    "        return Node(value = leaf_value)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features, model):\n",
    "        ''' function to find the best split '''\n",
    "        \n",
    "        # dictionary to store the best split\n",
    "        best_split = {}\n",
    "        best_split[\"info_gain\"] = 0\n",
    "        max_info_gain = 0\n",
    "\n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            \n",
    "            # ['Length', 'Lex_Prob_Scores_1gram', 'Lex_Prob_Scores_2gram', 'Lex_Prob_Scores_3gram', 'Syntactic_Prob_Scores']\n",
    "            cols = list(dataset.columns)\n",
    "            \n",
    "            print(\"\\n\\n\\tCurrently splitting over :\",list(dataset.columns)[feature_index])\n",
    "            \n",
    "            feature_values = list(dataset.iloc[:, feature_index])\n",
    "            \n",
    "            a = int(min(feature_values))\n",
    "            b = int(max(feature_values))\n",
    "            if cols[feature_index] in ['Length','Lex_Prob_Scores_2gram','Lex_Prob_Scores_3gram'] :\n",
    "                div = 1\n",
    "            else:\n",
    "                div = 2\n",
    "            \n",
    "            possible_thresholds = [i for i in range(a,b,div)]\n",
    "            print(\"\\tTotal no. of thersholds possible:\",len(possible_thresholds))\n",
    "            \n",
    "            # loop over all the feature values present in the data\n",
    "            for threshold in possible_thresholds:\n",
    "                \n",
    "                \n",
    "                # get current split\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                left_len = len(dataset_left)\n",
    "                right_len = len(dataset_right)\n",
    "                \n",
    "                # check if childs are not null\n",
    "                \n",
    "                if left_len>0 and right_len>0:\n",
    "                    y, left_y, right_y = dataset.iloc[:, -1], dataset_left.iloc[:, -1], dataset_right.iloc[:, -1]\n",
    "                    \n",
    "                    # compute information gain\n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y, model)\n",
    "                    \n",
    "                    \n",
    "                    # update the best split if needed\n",
    "                    if curr_info_gain > max_info_gain:\n",
    "                        print(\"\\n\\t\\tConsidering threshold as:\",threshold)\n",
    "                        print(\"\\t\\tleft_len:\",left_len,\"\\tright_len:\",right_len)\n",
    "                        print(\"\\t\\tCurrent Gain is: \",curr_info_gain,\"\\tMaximum gain is: \",max_info_gain)\n",
    "                        max_info_gain = curr_info_gain\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        \n",
    "        return best_split\n",
    "    \n",
    "    # to split dataset into two parts based on given feature and threshold\n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        ''' function to split the data '''\n",
    "        ds_L = []\n",
    "        ds_R = []\n",
    "        for i in range(len(dataset)):\n",
    "            row = dataset.iloc[i,:] \n",
    "            if row[feature_index] <= threshold:\n",
    "                ds_L.append(row)\n",
    "            else:\n",
    "                ds_R.append(row)\n",
    "\n",
    "        dataset_left = pd.DataFrame(ds_L)\n",
    "        dataset_right = pd.DataFrame(ds_R)\n",
    "        \n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    # calculate info gain based on type of model\n",
    "    def information_gain(self, parent, l_child, r_child, model):\n",
    "        ''' function to compute information gain '''\n",
    "        \n",
    "        weight_l = len(l_child) / len(parent)     # ni/n\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        if model == \"gini\":\n",
    "            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n",
    "        elif model == \"cross_entropy\":\n",
    "            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n",
    "        else:\n",
    "            gain = self.mce(parent) - (weight_l*self.mce(l_child) + weight_r*self.mce(r_child))\n",
    "        return gain\n",
    "    \n",
    "    # calculate gini_index\n",
    "    def gini_index(self, y):\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            gini += p_cls**2\n",
    "        return 1 - gini\n",
    "    \n",
    "    # calculate cross-entropy\n",
    "    def entropy(self, y):\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        return entropy\n",
    "    \n",
    "    # calculate misclassification error\n",
    "    def mce(self, y):\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        mce = 0\n",
    "        max_p = -float(\"inf\")\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            if p_cls > max_p:\n",
    "                max_p = p_cls\n",
    "        return 1 - max_p\n",
    "    \n",
    "    # compute leaf node class label\n",
    "    def calculate_leaf_value(self, Y):\n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "    \n",
    "    # start training tree with given data and model\n",
    "    def fit(self,dataset,model):\n",
    "        \n",
    "        self.root = self.build_tree(dataset,model,0)\n",
    "    \n",
    "    # predict new dataset class labels and return as list\n",
    "    def predict(self, X):\n",
    "        \n",
    "        preditions = []\n",
    "        for i in range(len(X)):\n",
    "            row = X.iloc[i,:]\n",
    "            preditions.append(self.make_prediction(row, self.root))\n",
    "        return preditions\n",
    "    \n",
    "    # predict a single data element's class label and return it \n",
    "    def make_prediction(self, row, tree):\n",
    "        \n",
    "        if tree.value != None: return tree.value\n",
    "        \n",
    "        feature_val = row[tree.feature_index]\n",
    "        if feature_val <= tree.threshold:\n",
    "            return self.make_prediction(row, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(row, tree.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8657250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the classifier object\n",
    "\n",
    "def get_trained_classfier(tr_df,model):\n",
    "    \n",
    "    classifier = DecisionTreeClassifier(min_samples_split=2, max_depth=10)\n",
    "    classifier.fit(tr_df,model)\n",
    "    return classifier\n",
    "\n",
    "\n",
    "# making predictions and creating confusion matrix\n",
    "\n",
    "def prediction_results(classifier,test_df):\n",
    "    \n",
    "    classes = list(np.unique(test_df['Class']))\n",
    "    class_actual = list(test_df.iloc[:,-1])\n",
    "    class_predicted = classifier.predict(test_df.iloc[:,:-1])\n",
    "\n",
    "    total = len(test_df)\n",
    "    confusion_matrix = pd.DataFrame(0, index = classes, columns = classes)\n",
    "\n",
    "    for i in range(total):\n",
    "        confusion_matrix.loc[class_actual[i],class_predicted[i]] += 1\n",
    "    prec = []\n",
    "    rec = []\n",
    "    f1 = []\n",
    "    sum_all = sum(sum(np.array(confusion_matrix.iloc[:,:])))\n",
    "    sum_tp = 0\n",
    "    for cls in classes:\n",
    "        tp = confusion_matrix.loc[cls,cls]\n",
    "        sum_tp += tp\n",
    "        ap = sum(confusion_matrix.loc[cls,:])\n",
    "        pp = sum(confusion_matrix.loc[:,cls])\n",
    "        if pp == 0:\n",
    "            p = 0\n",
    "        else:\n",
    "            p = tp/pp\n",
    "        if ap == 0:\n",
    "            r = 0\n",
    "        else:\n",
    "            r = tp/ap\n",
    "        if p+r == 0:\n",
    "            f = 0\n",
    "        else:\n",
    "            f = 2*p*r/(p+r)\n",
    "        prec.append(round(p*100,2))\n",
    "        rec.append(round(r*100,2))\n",
    "        f1.append(round(f*100,2))\n",
    "            \n",
    "    confusion_matrix.loc['Precision'] = prec\n",
    "    confusion_matrix.loc['Recall'] = rec\n",
    "    confusion_matrix.loc['f1_score'] = f1\n",
    "    accuracy = round(100*sum_tp/sum_all,2)\n",
    "    return (accuracy,confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cd5312",
   "metadata": {},
   "source": [
    "# Making required datsets for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc69bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating training and testing data based on feature combination set chosen\n",
    "lists = [[2,4,6,8,10,0],[2,4,6,8,0],[2,4,6,0],[2,4,0]]\n",
    "tr_df_list = []\n",
    "ts_df_list = []\n",
    "for i in range(len(lists)):\n",
    "    temp = total_df.iloc[0:5452,lists[i]].copy()\n",
    "    tr_df_list.append(temp)\n",
    "    temp = total_df.iloc[5452:5953,lists[i]].copy()\n",
    "    ts_df_list.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5c5d6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Lex_Prob_Scores_1gram</th>\n",
       "      <th>Lex_Prob_Scores_2gram</th>\n",
       "      <th>Lex_Prob_Scores_3gram</th>\n",
       "      <th>Syntactic_Prob_Scores</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>26.007478</td>\n",
       "      <td>3.226824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.242043</td>\n",
       "      <td>DESC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>12.625627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.475343</td>\n",
       "      <td>ENTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>53.465525</td>\n",
       "      <td>5.339126</td>\n",
       "      <td>17.262693</td>\n",
       "      <td>29.197039</td>\n",
       "      <td>DESC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>29.855507</td>\n",
       "      <td>1.126969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.417258</td>\n",
       "      <td>ENTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>28.578077</td>\n",
       "      <td>1.670732</td>\n",
       "      <td>3.132457</td>\n",
       "      <td>14.477990</td>\n",
       "      <td>ABBR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>9</td>\n",
       "      <td>20.895701</td>\n",
       "      <td>7.445157</td>\n",
       "      <td>2.726126</td>\n",
       "      <td>23.981544</td>\n",
       "      <td>ENTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>8</td>\n",
       "      <td>36.319544</td>\n",
       "      <td>7.074357</td>\n",
       "      <td>2.619847</td>\n",
       "      <td>17.918298</td>\n",
       "      <td>ENTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5449</th>\n",
       "      <td>5</td>\n",
       "      <td>8.542456</td>\n",
       "      <td>1.670732</td>\n",
       "      <td>3.132457</td>\n",
       "      <td>7.550124</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>6</td>\n",
       "      <td>13.246662</td>\n",
       "      <td>1.670732</td>\n",
       "      <td>3.132457</td>\n",
       "      <td>9.910685</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5451</th>\n",
       "      <td>6</td>\n",
       "      <td>24.808424</td>\n",
       "      <td>1.366251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.819242</td>\n",
       "      <td>ENTY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5452 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Length  Lex_Prob_Scores_1gram  Lex_Prob_Scores_2gram  \\\n",
       "0          9              26.007478               3.226824   \n",
       "1          7              12.625627               0.000000   \n",
       "2         10              53.465525               5.339126   \n",
       "3         12              29.855507               1.126969   \n",
       "4          7              28.578077               1.670732   \n",
       "...      ...                    ...                    ...   \n",
       "5447       9              20.895701               7.445157   \n",
       "5448       8              36.319544               7.074357   \n",
       "5449       5               8.542456               1.670732   \n",
       "5450       6              13.246662               1.670732   \n",
       "5451       6              24.808424               1.366251   \n",
       "\n",
       "      Lex_Prob_Scores_3gram  Syntactic_Prob_Scores Class  \n",
       "0                  0.000000              18.242043  DESC  \n",
       "1                  0.000000               6.475343  ENTY  \n",
       "2                 17.262693              29.197039  DESC  \n",
       "3                  0.000000              30.417258  ENTY  \n",
       "4                  3.132457              14.477990  ABBR  \n",
       "...                     ...                    ...   ...  \n",
       "5447               2.726126              23.981544  ENTY  \n",
       "5448               2.619847              17.918298  ENTY  \n",
       "5449               3.132457               7.550124   NUM  \n",
       "5450               3.132457               9.910685   NUM  \n",
       "5451               0.000000              13.819242  ENTY  \n",
       "\n",
       "[5452 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e91df57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Length\n",
      "\tMin: 2\n",
      "\tMax: 33\n",
      "\tUnique: 28\n",
      "For Lex_Prob_Scores_1gram\n",
      "\tMin: 0.0\n",
      "\tMax: 145.28172318224554\n",
      "\tUnique: 4437\n",
      "For Lex_Prob_Scores_2gram\n",
      "\tMin: 0.0\n",
      "\tMax: 22.95301614402827\n",
      "\tUnique: 1857\n",
      "For Lex_Prob_Scores_3gram\n",
      "\tMin: 0.0\n",
      "\tMax: 32.44418642051975\n",
      "\tUnique: 410\n",
      "For Syntactic_Prob_Scores\n",
      "\tMin: 0.0\n",
      "\tMax: 132.7605092643911\n",
      "\tUnique: 3290\n"
     ]
    }
   ],
   "source": [
    "explore_features(tr_df_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6817fdb",
   "metadata": {},
   "source": [
    "# Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c72bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying kfold for training data and getting evaluation metrics\n",
    "\n",
    "dataset = tr_df_list[0]\n",
    "train_set, validn_set = apply_k_fold(dataset,10)\n",
    "\n",
    "classf_list = []\n",
    "result_kfolds = []\n",
    "\n",
    "mod = 'gini'\n",
    "\n",
    "for i in range(1, 10):\n",
    "    \n",
    "    print(\"\\n\\n\\n********************* Kfold iteration\",i,\"*********************\\n\\n\")\n",
    "    train_set_i = dataset.iloc[list(train_set[i]),:]\n",
    "    validation_set_i = dataset.iloc[list(validn_set[i\n",
    "                                                    \n",
    "    c = get_trained_classfier(train_set_i,mod)\n",
    "    classf_list.append(c)\n",
    "                                                    \n",
    "    r = prediction_results(c,validation_set_i)\n",
    "    result_kfolds.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da03e302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting evaluation metrics for all three models over different combination of features\n",
    "\n",
    "model_set = ['gini','misclsfn_err','cross_entropy']\n",
    "# model_scores = {}                # (key,value) = (accuracy,confusion matrix)\n",
    "classifers_model_wise = []\n",
    "results_model_wise = []\n",
    "for model in model_set:\n",
    "    print(\"\\n\\n\\n\\n\\n*********************************** For \",model,\"index *****************************************\\n\\n\\n\\n\\n\")\n",
    "    classifers_feature_wise = []\n",
    "    results_feature_wise = []\n",
    "#     if model == 'gini':\n",
    "#         skip = 1\n",
    "    for i in [0,1,2,3]:\n",
    "#         if i in [1,2,3] and skip == 1:\n",
    "#             continue\n",
    "        print(\"\\n\\n********************* SET \",i,\"*********************\\n\\n\\n\\n\\n\")\n",
    "        clsfier = get_trained_classfier(tr_df_list[i],model)\n",
    "        classifers_feature_wise.append(clsfier)\n",
    "        \n",
    "        res = prediction_results(clsfier,ts_df_list[i])\n",
    "        results_feature_wise.append(res)\n",
    "    \n",
    "    classifers_model_wise.append(classifers_feature_wise)\n",
    "    results_model_wise.append(results_feature_wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f29793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating all possible combinations of feature indexes\n",
    "li = [2,4,6,8,10]\n",
    "list_comb = []\n",
    "for L in range(0, len(li)+1):\n",
    "    for subset in itertools.combinations(li, L):\n",
    "        if len(subset) > 1:\n",
    "            lis = list(subset)\n",
    "            lis.append(0)\n",
    "            list_comb.append(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a0b4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking feature ablation results\n",
    "cols = list(total_df.columns)\n",
    "feature_ablation_results =[]\n",
    "for lis in list_comb:\n",
    "#     print(\"Considering feature set\")\n",
    "#     for index in lis:\n",
    "#         print(cols[])\n",
    "    print(\"\\n******************************* Considering list set\",lis,\"**************************\\n\\n\")\n",
    "    trds = total_df.iloc[0:5452,lis].copy()\n",
    "    tsds = total_df.iloc[5452:5953,lis].copy()\n",
    "    feature_ablation_results.append(prediction_results(get_trained_classfier(trds,'gini'),tsds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
